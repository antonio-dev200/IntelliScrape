### **IntelliScrape 系统架构设计文档**

| 版本 | 日期 | 作者 | 状态 |
| :---- | :---- | :---- | :---- |
| V1.0 | 2025年8月13日 | Gemini | 最终版 |

#### **1\. 引言**

本文档为IntelliScrape平台提供最终的技术实现方案，旨在指导后续的开发工作。本文档的核心是**将AI分析与人工标准化解耦**，并为MVP（最小可行产品）阶段定义了最简化的部署和技术栈。

#### **2\. 总体架构图 (MVP简化版)**

graph TD  
    subgraph 用户端 (User Facing)  
        WebApp\[Web管理后台\<br\>(React/Vue)\]  
    end

    subgraph 核心后端 (Core Backend)  
        BFF\[API网关/BFF\<br\>(Python/FastAPI)\]  
        Orchestrator\[任务编排服务\<br\>(Python/Celery)\]  
    end

    subgraph AI与数据处理 (AI & Data Processing)  
        DiscoverySvc\[模式发现服务\<br\>(Python/FastAPI \+ AI Libs)\]  
        ExtractorSvc\[数据提取服务\<br\>(Python/Scrapy-Playwright)\]  
        AnalysisSvc\[智能分析服务\<br\>(Python/FastAPI)\]  
    end

    subgraph 数据存储 (Data Persistence)  
        AppDatabase\[(应用数据库\<br\>PostgreSQL)\]  
        MessageQueue\[(消息队列\<br\>RabbitMQ/Kafka)\]  
        LLM\[外部大语言模型 API\]  
    end

    %% 连接关系  
    WebApp \--\> BFF  
    BFF \--\> Orchestrator  
    BFF \--\> AnalysisSvc  
    BFF \--\> AppDatabase

    Orchestrator \-- "异步API调用" \--\> DiscoverySvc  
    Orchestrator \--\> MessageQueue  
    Orchestrator \--\> AppDatabase

    MessageQueue \-- "数据提取任务" \--\> ExtractorSvc

    DiscoverySvc \--\> AppDatabase  
    DiscoverySvc \-- "调用AI分析" \--\> LLM

    ExtractorSvc \--\> AppDatabase  
    AnalysisSvc \--\> AppDatabase  
    AnalysisSvc \-- "调用AI生成报告" \--\> LLM

#### **3\. 核心服务设计**

1. **任务编排服务 (Task Orchestrator)**  
   * **职责:** 负责所有后台任务的**触发与分发**。  
   * **核心逻辑:**  
     * **执行抓取任务:** 接收Crawl Task ID，查询其关联的data\_source\_ids，为每个来源查找已激活的Crawl Config，并将包含config\_id的子任务推送到extraction\_queue。  
     * **触发AI分析:** 接收来自BFF的分析请求（包含data\_source\_id和theme\_name），并向Discovery Service发起异步API调用。  
2. **模式发现服务 (Discovery Service)**  
   * **职责:** **单一职责** \- 接收分析请求，调用AI模型，将一个网站上发现的**所有原始字段和抓取规则**存入raw\_analysis\_results表。它不关心标准化，只负责“发现”。  
3. **API网关/BFF**  
   * **职责:** 作为前端的统一入口，并**承载“标准化”的核心业务逻辑**。  
   * **核心逻辑:**  
     * 提供API，供前端发起“主题配置”流程。  
     * 提供API，用于从raw\_analysis\_results读取多个数据源的分析结果，并执行归并、推荐等业务逻辑。  
     * 提供API，接收管理员“标准化”后的最终配置，并在一个数据库事务中完成对standard\_datasets, standard\_fields和crawl\_configs的写入或更新。  
4. **数据提取服务 (Extractor Service)**  
   * **职责:** **单一职责** \- 消费extraction\_queue中的消息，加载指定的Crawl Config，执行抓取并将数据存入数据库。

#### **4\. 数据存储设计 (优化版)**

1. **应用数据库 (Application DB) \- PostgreSQL**  
   * **standard\_datasets**: (id, name, description, ...)  
   * **standard\_fields**: (id, dataset\_id, field\_name, description, ...)  
   * **data\_sources**: (id, site\_key, name, ...)  
   * **raw\_analysis\_results (中间表)**: (id, data\_source\_id, theme\_name, raw\_fields\_json, status)  
   * **crawl\_configs**: (id, data\_source\_id, standard\_dataset\_id, version, field\_selectors\_json, status, ...)  
     * field\_selectors\_json 结构: {"mappings": \[{"standard\_field\_id": 1, "selector": ".title"}\], "extra\_fields": \[{"field\_name": "rating", "selector": ".rating"}\]}  
   * **crawl\_tasks**: (id, name, standard\_dataset\_id, data\_source\_ids, schedule\_type, status, ...)  
   * **抓取数据表:** 为每个standard\_dataset动态创建一张数据表，包含所有标准字段和一个extra\_data (JSONB)字段。

#### **5\. 关键工作流程 (最终版)**

1. **工作流一：配置或更新数据主题**  
   * **阶段一：批量分析 (全自动)**  
     1. **发起流程:** 管理员在后台发起“主题配置”，输入主题名称（如“公司财报”），并勾选所有需要新增或更新的数据源（网站A、B、C）。  
     2. **触发分析:** BFF向Orchestrator为每个网站发起分析请求。  
     3. **存储原始结果:** Orchestrator调用Discovery Service，服务完成后，将每个网站的原始分析结果存入raw\_analysis\_results表。  
   * **阶段二：人工标准化 (用户交互)**  
     1. **进入工作台:** 管理员进入“标准化工作台”，选择“公司财报”主题。  
     2. **加载与归并:** BFF从数据库加载该主题已有的标准和本次新分析出的原始字段，执行归并与智能推荐。  
     3. **审核与确认:** 管理员在界面上完成映射、提升、忽略等操作。  
     4. **生成/更新配置:** BFF接收到最终确认的指令，在一个数据库事务中，完成对standard\_datasets, standard\_fields和crawl\_configs的写入或更新，并将crawl\_configs的状态设为active。  
2. **工作流二：执行数据采集任务**  
   1. **创建任务:** 管理员创建一个Crawl Task，关联到“公司财报”这个Standard Dataset，并勾选已配置好的网站A和C。  
   2. **调度与分发:** Orchestrator根据计划触发任务，遍历网站A和C，找到它们已激活的Crawl Config，并将包含config\_id的子任务推送到extraction\_queue。  
   3. **提取数据:** Extractor Service消费队列，加载配置，执行抓取，存入数据。

#### **6\. 部署与运维 (MVP简化版)**

* **部署:** 所有服务将直接部署在开发或测试服务器上，每个服务作为一个独立的Python进程运行。  
* **环境管理:** 使用Python的虚拟环境（如venv）和requirements.txt来管理依赖。  
* **密钥管理:** 数据库密码、外部API密钥等敏感信息，通过环境变量进行管理。  
* **未来演进:** 待系统稳定后，再引入Docker和Kubernetes进行容器化部署。